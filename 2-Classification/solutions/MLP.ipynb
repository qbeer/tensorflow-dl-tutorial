{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multilayer Perceptron\n",
    "\n",
    "[Similar tutorial](https://www.tensorflow.org/get_started/mnist/pros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cXBoqDwV2z3V"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "biXxnT2wRQpo"
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset can be automatically downloaded via tensorflow with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "output_extras": [
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 947,
     "status": "ok",
     "timestamp": 1510073056333,
     "user": {
      "displayName": "David Nagy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116820342271745675144"
     },
     "user_tz": -60
    },
    "id": "-R0HUxwNHvI6",
    "outputId": "56b00892-486c-4759-d82c-6ef8003164a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try plotting one of the datapoints (from mnist.train.images) with matplotlib's `plt.imshow`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 265,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 542,
     "status": "ok",
     "timestamp": 1510073182906,
     "user": {
      "displayName": "David Nagy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116820342271745675144"
     },
     "user_tz": -60
    },
    "id": "yuyt7k6nCZUn",
    "outputId": "362461f8-59fe-47ae-975e-d881289169ec"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP0tkhIQ9XsmHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121d1cdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.reshape(-mnist.train.images[1], (28, 28)), interpolation='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pG9Gd1JIRf0z"
   },
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define placeholders for the data and labels, however now instead of feeding in one datapoint at a time, define the placeholders for minibatches of size 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a two-layer perceptron with a third output layer, where each layer is a linear transformation followed by a sigmoid nonlinearity:\n",
    "\n",
    "$$y=\\sigma_{softmax}\\circ A_2 \\circ \\sigma \\circ A_1\\circ \\mathbf{W} $$\n",
    "\n",
    "where \n",
    "- $A_i(x)=\\mathbf{W}_i \\mathbf{x}+\\mathbf{b}_i$,\n",
    "- $\\sigma$ is a [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) function (`tf.nn.sigmoid`) and\n",
    "- $\\sigma_{softmax}$ is a [softmax](https://en.wikipedia.org/wiki/Softmax_function) function (`tf.nn.softmax`).\n",
    "\n",
    "As initial values for the parameters, use a truncated normal with `stddev = 0.1` for the weight and a constant `0.1` for the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rSIrT-Bx26_I"
   },
   "outputs": [],
   "source": [
    "# inferred variables (variables)\n",
    "h1_size = 100\n",
    "W1 = tf.Variable(tf.truncated_normal([784, h1_size], stddev=0.1))\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[h1_size]))\n",
    "h1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([h1_size, 10], stddev=0.1))\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "h2 = tf.matmul(h1, W2) + b2\n",
    "\n",
    "y = tf.nn.softmax(h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8k9dah0HRh08"
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a cross-entropy loss function \n",
    "$$-\\sum y_{true} \\cdot \\log y$$\n",
    "and an optimizer. The loss should be minimised on average over the minibatch. Try both the gradient descent optimizer from the previous excercise and the [Adam](https://arxiv.org/abs/1412.6980) optimizer. There is [possible numerical instability](https://deepnotes.io/softmax-crossentropy) of the loss function if it is defined directly with the above equation, which can be cirumvented for example by using the built-in `tf.nn.softmax_cross_entropy_with_logits` in the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "FTZOUQDAGzz0"
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y)))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=h2))\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.25).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DWhv9Tj0RjNJ"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, write a training loop where you initialize the variables and iterate over minibatches of data. You can get the next minibatch of size N with:\n",
    "```\n",
    "x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "```\n",
    "Print out the loss, train and test errors at every n-th step, either to stdout or to tensorboard. As a guide, test accuracy of at least 97% is possible with this setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 357,
     "output_extras": [
      {
       "item_id": 5
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1655,
     "status": "ok",
     "timestamp": 1510073307648,
     "user": {
      "displayName": "David Nagy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116820342271745675144"
     },
     "user_tz": -60
    },
    "id": "6efyKepg3iGM",
    "outputId": "03935023-0d62-45d9-ff84-073323c3417b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.2778\n",
      "train acc: 0.2794\n",
      "test acc: 0.9416\n",
      "train acc: 0.944855\n",
      "test acc: 0.9538\n",
      "train acc: 0.960218\n",
      "test acc: 0.9657\n",
      "train acc: 0.972418\n",
      "test acc: 0.9652\n",
      "train acc: 0.975055\n",
      "test acc: 0.9698\n",
      "train acc: 0.983327\n",
      "test acc: 0.97\n",
      "train acc: 0.9852\n",
      "test acc: 0.9712\n",
      "train acc: 0.989491\n",
      "test acc: 0.9715\n",
      "train acc: 0.989945\n",
      "test acc: 0.9736\n",
      "train acc: 0.992491\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(2000):\n",
    "        x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        loss_, _ = sess.run((loss, optimizer), feed_dict={x: x_batch, y_true:y_batch })\n",
    "        if (i % 200 == 0):\n",
    "            #print(\"loss:\", loss_)\n",
    "            correct = tf.equal(tf.argmax(y,1), tf.argmax(y_true,1))\n",
    "            print(\"test acc:\", sess.run(tf.reduce_mean(tf.cast(correct, tf.float32)),\n",
    "                           feed_dict={x: mnist.test.images, y_true: mnist.test.labels}))\n",
    "            print(\"train acc:\", sess.run(tf.reduce_mean(tf.cast(correct, tf.float32)),\n",
    "                           feed_dict={x: mnist.train.images, y_true: mnist.train.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Multilayer perceptron.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nteract": {
   "version": "0.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
