{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Auto-Encoding Variational Bayes (Kingma, Welling, 2013)](https://arxiv.org/abs/1312.6114)\n",
    "\n",
    "Solution is based on [Oliver DÃ¼rr's](https://github.com/oduerr/dl_tutorial/tree/master/tensorflow/vae)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "K__ySwAGTwtA"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the MNIST dataset in the same way as for the perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XXLxdpyUTwtW"
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a placeholder called `x` for data minibatches of size 64. Then create two fully connected layers in the same way as for the classifier, but here in the last layer, instead of a softmax, we should map with a linear trainsformation into the parameters of the gaussian posterior. \n",
    "\n",
    "$$h_2(x)=\\sigma\\circ A_2\\circ \\sigma \\circ A_1\\circ x$$\n",
    "\n",
    "$$\\mu=A_{3m}\\circ h_2(x)$$\n",
    "\n",
    "$$\\sigma_{pre}=A_{3v}\\circ h_2(x)$$\n",
    "\n",
    "These should for now be two `n_z` dimensional vector variables named `z_mean` and `z_sigma_pre`. Try `n_z = 2` and `n_h = 500` for the hidden layer widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the sampling from the variational posterior, where we need to generate z samples for the whole minibatch. Generate an epsilon from a `tf.random_normal` with zero mean and unit variance, then:\n",
    "\n",
    "$$z=\\mu+\\sqrt{e^{\\sigma_{pre}}}\\cdot \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ONyo-8ZATwtZ"
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative model or decoder takes the latent $z$ values as inputs and generates datapoints from them. Define a two-layer fully connected network again to use as a generative model of digits. The last layer should map linearly into a vector of mean pixel values named `x_reconstr_mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmPvT_ZeTwte"
   },
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ELBO loss consists of the reconstruction and regularization terms. The first is the expected value of the log pdf of data conditioned on the inferred z value, the second is the kl divergence between the inferred posterior and the prior. The KL term can be calculated analytically for the case where both the prior and the posterior are gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QdC1s3eCTwti"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training will need a bit more time so I've provided a training code skeleton with a progress bar.\n",
    "\n",
    "```\n",
    "runs = 10\n",
    "n_minibatches = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(runs):\n",
    "    pbar = tf.contrib.keras.utils.Progbar(n_minibatches)\n",
    "    for i in range(n_minibatches):\n",
    "    \n",
    "        #YOUR CODE HERE - update and calculate \"cost_\" on a minibatch\n",
    "        \n",
    "        pbar.add(1,[(\"cost\",cost_)])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to leave the training running for a longer while and save the resulting model for later analysis, you can load the tensorflow saver with `saver = tf.train.Saver()`, and save the current state with `saver.save(sess, \"./model.ckpt\")`. You can later restore the saved parameters with `saver.restore(sess, check_point_file)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5G7heLTTwtm"
   },
   "source": [
    "## Load and analyse model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some reconstructions. You can use the following code for a plot grid:\n",
    "    \n",
    "```\n",
    "n_rec = 10\n",
    "plt.figure(figsize=(n_rec+2,4))\n",
    "for i in range(n_rec):\n",
    "    plt.subplot(2, n_rec, i+1)\n",
    "    \n",
    "    #YOUR CODE HERE - plot command for samples\n",
    "    \n",
    "    plt.subplot(2, n_rec, n_rec+i+1)\n",
    "    \n",
    "    #YOUR CODE HERE - plot command for reconstructions\n",
    "    \n",
    "plt.tight_layout()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the z space embedding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the latent space by plotting images conditioned on a grid in $z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "vae2dmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nteract": {
   "version": "0.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
